{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fyDgiGYjqQVYtQ5lVZI6pC_HhoQ02vMG","timestamp":1742572387502},{"file_id":"1KBV_sRbpKkB2t_EXk8h8dm-sARxrAyeV","timestamp":1709802864030}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"f5UGU6F4pLWM"}},{"cell_type":"markdown","source":["# Motion History Image (MHI)\n","One of the key techniques pre-deep learning for representing and analysing motion in video sequences was the concept of Motion History Images (MHI).\n","\n","## What is a Motion History Image?\n","A Motion History Image is a visual method for representing temporal motion in a video sequence. MHI was proposed by Bobick and Davis:\n","* Bobick, A. F., & Davis, J. W. (2001). [The recognition of human movement using temporal templates](https://www.cs.bu.edu/fac/betke/cs591/papers/bobick-davis.pdf). *IEEE Transactions on Pattern Analysis and Machine Intelligence, 23*(3), 257-267.\n","\n","It is essentially a single image that encapsulates information about the movement of objects over time. The core idea behind MHI is to provide a compact representation of motion by blending a sequence of frames into a single image, where the intensity of each pixel is a function of the recency of motion at that location.\n","\n","## How MHI Works\n","The MHI is generated by processing a series of video frames and updating each pixel's intensity level based on the motion detected at that pixel. When motion occurs at a particular pixel location, the value of that pixel in the MHI is reset to a maximum value, which then gradually decays over time in the absence of new motion. As a result, the resulting MHI encodes the temporal history of motion in a way that regions with recent movement appear brighter, while older movements fade into darker regions.\n","\n","## Applications of MHI\n","MHI finds its utility in various domains:\n","1. Action Recognition: MHIs provide a means to capture the essence of an action performed by a person or an object, making them suitable for action recognition tasks.\n","2. Gesture Analysis: In human-computer interaction, MHIs can be employed to understand and interpret gestures by analyzing the motion trajectory over time.\n","3. Surveillance: In security and surveillance, MHIs help in detecting and summarizing movements in a monitored area, aiding in anomaly detection or activity analysis.\n","4. Sports Analysis: Coaches and athletes can use MHIs for technique assessment, where the motion patterns in sports activities are analyzed for performance improvement.\n","\n","## Advantages and Challenges\n","MHIs offer a simple yet effective way of summarizing motion, providing an intuitive representation that is less complex compared to processing the full video data. However, the effectiveness of MHIs can be influenced by factors like the duration of history captured, the rate of decay of the pixel intensities, and the complexity of the background in the video."],"metadata":{"id":"llUl5DXnQn7_"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"oFzCwtFzp2vs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAFmIRxzQna-"},"outputs":[],"source":["!pip install opencv-python numpy matplotlib"]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt"],"metadata":{"id":"M8Vlx61PQ2pp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Obtain image samples\n","\n","These images have been obtained from the [NTU RGB+D dataset](https://rose1.ntu.edu.sg/dataset/actionRecognition)."],"metadata":{"id":"LxmYbCiIqKDU"}},{"cell_type":"code","source":["!wget 'https://github.com/FranciscoFlorezRevuelta/HAR/raw/main/samples/S001C001P001R001A055_rgb.avi' -O example1.avi\n","!wget 'https://github.com/FranciscoFlorezRevuelta/HAR/raw/main/samples/S001C001P001R002A015_rgb.avi' -O example2.avi\n","!wget 'https://github.com/FranciscoFlorezRevuelta/HAR/raw/main/samples/S001C002P006R001A050_rgb.avi' -O example3.avi\n","!wget 'https://github.com/FranciscoFlorezRevuelta/HAR/raw/main/samples/S001C002P006R001A051_rgb.avi' -O example4.avi\n","!wget 'https://github.com/FranciscoFlorezRevuelta/HAR/raw/main/samples/S001C002P006R001A052_rgb.avi' -O example5.avi\n","!wget 'https://github.com/FranciscoFlorezRevuelta/HAR/raw/main/samples/S001C002P007R002A048_rgb.avi' -O example6.avi"],"metadata":{"id":"wt87UJstViOv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Read the video\n","\n","You can change the name of the video to example{1-6}.avi"],"metadata":{"id":"fWKbvCWmrEHy"}},{"cell_type":"code","source":["# Load your video\n","video_path = '/content/example1.avi'\n","cap = cv2.VideoCapture(video_path)"],"metadata":{"id":"8o2QMiWfVEMR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialise MHI parameters"],"metadata":{"id":"MJEdpopFri-J"}},{"cell_type":"code","source":["# Initialize MHI parameters\n","duration = 100\n","timestamp = 0\n","ret, prev_frame = cap.read()\n","prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","h, w = prev_frame.shape\n","mhi = np.zeros((h, w), dtype=np.float32)\n","plt.imshow(prev_frame,cmap='gray')\n","plt.show()"],"metadata":{"id":"HYq9Ev3iVGCv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculate motion"],"metadata":{"id":"OjFH974nrv9d"}},{"cell_type":"code","source":["def calculate_motion(prev_frame, curr_frame, mhi, timestamp, duration):\n","    \"\"\" Calculate the difference between frames and update the MHI. \"\"\"\n","    # Calculate the absolute difference between the current frame and the previous frame\n","    motion = cv2.absdiff(prev_frame, curr_frame)\n","    _, thresh = cv2.threshold(motion, 25, 1, cv2.THRESH_BINARY)\n","\n","    # Update MHI: decrease the intensity for older movements and increase for new movements\n","    mhi[mhi > 0] -= 1  # Decaying the MHI\n","    mhi[thresh == 1] = duration  # Updating new motion\n","\n","    return mhi"],"metadata":{"id":"lZFD5jMaVARn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Process all frames"],"metadata":{"id":"rJWZOM9jr1g7"}},{"cell_type":"code","source":["while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    mhi = calculate_motion(prev_frame, frame, mhi, timestamp, duration)\n","    prev_frame = frame.copy()"],"metadata":{"id":"zzYnZ-06VIFx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Normalise the MHI for visualisation"],"metadata":{"id":"tHxgOcQhr8H-"}},{"cell_type":"code","source":["# Normalize the MHI for visualization\n","mhi = np.uint8(np.clip((mhi / duration) * 255, 0, 255))\n","plt.imshow(mhi, cmap='gray')\n","plt.show()\n","\n","cap.release()"],"metadata":{"id":"xhbZnBxXVQes"},"execution_count":null,"outputs":[]}]}