{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[{"file_id":"1dz62o9MROdxW3ZZimeZ3xEN9JxqHBED8","timestamp":1742572396290},{"file_id":"1yM--K03ZvSiEi8Pudj5Vj_pWiZ6GI__y","timestamp":1709545280763},{"file_id":"https://github.com/michalfaber/keras_Realtime_Multi-Person_Pose_Estimation/blob/master/demo.ipynb","timestamp":1573491822130}]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hPg95cQLwckQ"},"source":["# Human pose estimation\n","   \n","![alt text](https://github.com/ZheC/Multi-Person-Pose-Estimation/raw/master/readme/shake.gif)\n","\n","This notebook is based on: https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation and adapted from https://colab.research.google.com/drive/1yM--K03ZvSiEi8Pudj5Vj_pWiZ6GI__y.\n","\n","**OpenPose** is an advanced method for real-time multi-person keypoint detection, pioneering the field of real-time 2D pose estimation. Developed by the Perceptual Computing Lab at Carnegie Mellon University, it represents a significant advancement in human pose estimation technology.\n","\n","> Cao, Z., Simon, T., Wei, S. E., & Sheikh, Y. (2017). [Realtime multi-person 2d pose estimation using part affinity fields](http://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.pdf). In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7291-7299).\n","\n","## Core Principles\n","* **Convolutional Neural Networks (CNNs):** OpenPose utilizes deep learning, specifically CNNs, to detect human body parts and joints. The network is trained on large datasets containing images of people in various poses to learn how to identify body parts and joints accurately.\n","* **Multi-Person Pose Estimation:** Unlike some earlier methods that only worked for individual figures, OpenPose can detect multiple people in a single image, identifying the pose of each person independently.\n","* **Bottom-Up Approach:** OpenPose adopts a bottom-up approach, which means it first detects all instances of each body part (like all hands, all feet, etc.) and then assembles them into individual poses. This approach is generally faster than top-down methods (which first detect people and then estimate their pose).\n","\n","## Key Steps in OpenPose\n","* **Part Confidence Maps:** The network generates \"confidence maps\" for body parts. Each map is an image where the intensity of each pixel indicates the probability of a particular body part being at that location.\n","* **Part Affinity Fields (PAFs):** OpenPose introduces the concept of PAFs, which are vector fields that encode the degree of association between different body parts. For instance, they capture the information about which hand connects to which elbow.\n","* **Bipartite Graph Matching:** The system then uses the confidence maps and PAFs to create a bipartite graph, matching the detected body parts in a way that forms valid human poses.\n","* **Assembly of Keypoints into Poses:** Finally, the algorithm assembles the detected body parts into individual human poses for all the people in the image.\n","\n","## Advancements and Applications\n","* OpenPose handles a variety of keypoints, including facial landmarks and finger joints, which makes it highly versatile for detailed human pose estimation.\n","* It's used in numerous applications like gesture recognition, activity analysis, augmented reality, and human-computer interaction.\n","\n","#Limitations\n","* Despite its strengths, OpenPose can be computationally intensive, especially when processing high-resolution images or videos in real-time.\n","* The accuracy of pose estimation can be affected by factors like occlusion, lighting conditions, and complex human poses.\n"]},{"cell_type":"code","metadata":{"id":"orJu7Ol3Hj00"},"source":["# look at https://modelzoo.co/model/keras-realtime-multi-person-pose-estimation\n","!git clone https://github.com/kevinlin311tw/keras-openpose-reproduce.git\n","!mv keras-openpose-reproduce/* ./    &&    mkdir model/keras\n","!cd model   && sh get_keras_model.sh\n","!pip3 -q install Cython scikit-image pandas zmq h5py opencv-python configobj\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAkua2m5He5n"},"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Activation, Lambda\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Concatenate\n","from config_reader import config_reader\n","import scipy\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e7vbaGEHHe52"},"source":["### Definition of model\n","\n","Helper functions to create a model.  These are the individual blocks."]},{"cell_type":"code","metadata":{"id":"JIBhsHw4He54"},"source":["def relu(x):\n","    return Activation('relu')(x)\n","\n","def conv(x, nf, ks, name):\n","    x1 = Conv2D(nf, (ks, ks), padding='same', name=name)(x)\n","    return x1\n","\n","def pooling(x, ks, st, name):\n","    x = MaxPooling2D((ks, ks), strides=(st, st), name=name)(x)\n","    return x\n","\n","def vgg_block(x):\n","\n","    # Block 1\n","    x = conv(x, 64, 3, \"conv1_1\")\n","    x = relu(x)\n","    x = conv(x, 64, 3, \"conv1_2\")\n","    x = relu(x)\n","    x = pooling(x, 2, 2, \"pool1_1\")\n","\n","    # Block 2\n","    x = conv(x, 128, 3, \"conv2_1\")\n","    x = relu(x)\n","    x = conv(x, 128, 3, \"conv2_2\")\n","    x = relu(x)\n","    x = pooling(x, 2, 2, \"pool2_1\")\n","\n","    # Block 3\n","    x = conv(x, 256, 3, \"conv3_1\")\n","    x = relu(x)\n","    x = conv(x, 256, 3, \"conv3_2\")\n","    x = relu(x)\n","    x = conv(x, 256, 3, \"conv3_3\")\n","    x = relu(x)\n","    x = conv(x, 256, 3, \"conv3_4\")\n","    x = relu(x)\n","    x = pooling(x, 2, 2, \"pool3_1\")\n","\n","    # Block 4\n","    x = conv(x, 512, 3, \"conv4_1\")\n","    x = relu(x)\n","    x = conv(x, 512, 3, \"conv4_2\")\n","    x = relu(x)\n","\n","    # Additional non vgg layers\n","    x = conv(x, 256, 3, \"conv4_3_CPM\")\n","    x = relu(x)\n","    x = conv(x, 128, 3, \"conv4_4_CPM\")\n","    x = relu(x)\n","\n","    return x\n","\n","def stage1_block(x, num_p, branch):\n","\n","    # Block 1\n","    x = conv(x, 128, 3, \"conv5_1_CPM_L%d\" % branch)\n","    x = relu(x)\n","    x = conv(x, 128, 3, \"conv5_2_CPM_L%d\" % branch)\n","    x = relu(x)\n","    x = conv(x, 128, 3, \"conv5_3_CPM_L%d\" % branch)\n","    x = relu(x)\n","    x = conv(x, 512, 1, \"conv5_4_CPM_L%d\" % branch)\n","    x = relu(x)\n","    x = conv(x, num_p, 1, \"conv5_5_CPM_L%d\" % branch)\n","\n","    return x\n","\n","def stageT_block(x, num_p, stage, branch):\n","\n","    # Block 1\n","    x = conv(x, 128, 7, \"Mconv1_stage%d_L%d\" % (stage, branch))\n","    x = relu(x)\n","    x = conv(x, 128, 7, \"Mconv2_stage%d_L%d\" % (stage, branch))\n","    x = relu(x)\n","    x = conv(x, 128, 7, \"Mconv3_stage%d_L%d\" % (stage, branch))\n","    x = relu(x)\n","    x = conv(x, 128, 7, \"Mconv4_stage%d_L%d\" % (stage, branch))\n","    x = relu(x)\n","    x = conv(x, 128, 7, \"Mconv5_stage%d_L%d\" % (stage, branch))\n","    x = relu(x)\n","    x = conv(x, 128, 1, \"Mconv6_stage%d_L%d\" % (stage, branch))\n","    x = relu(x)\n","    x = conv(x, num_p, 1, \"Mconv7_stage%d_L%d\" % (stage, branch))\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KBfNiKypZSU7"},"source":["![openpose architecture](https://github.com/ZheC/Multi-Person-Pose-Estimation/raw/master/readme/arch.png)"]},{"cell_type":"markdown","metadata":{"id":"36aqNAPsHe58"},"source":["Create keras model by assembling the blocks and load the pretrained weights."]},{"cell_type":"code","metadata":{"id":"WITs3qSWHe59"},"source":["weights_path = \"model/keras/model.h5\" # original weights converted from caffe\n","# weights_path = \"training/weights.best.h5\" # weights traiined from scratch\n","\n","input_shape = (None,None,3)\n","\n","img_input = Input(shape=input_shape)\n","\n","stages = 6\n","np_branch1 = 38\n","np_branch2 = 19\n","\n","img_normalized = Lambda(lambda x: x / 256 - 0.5)(img_input)  # [-0.5, 0.5]\n","\n","# VGG\n","stage0_out = vgg_block(img_normalized)\n","\n","# stage 1\n","stage1_branch1_out = stage1_block(stage0_out, np_branch1, 1)\n","stage1_branch2_out = stage1_block(stage0_out, np_branch2, 2)\n","x = Concatenate()([stage1_branch1_out, stage1_branch2_out, stage0_out])\n","\n","# stage t >= 2\n","for sn in range(2, stages + 1):\n","    stageT_branch1_out = stageT_block(x, np_branch1, sn, 1)\n","    stageT_branch2_out = stageT_block(x, np_branch2, sn, 2)\n","    if (sn < stages):\n","        x = Concatenate()([stageT_branch1_out, stageT_branch2_out, stage0_out])\n","\n","model = Model(img_input, [stageT_branch1_out, stageT_branch2_out])\n","model.load_weights(weights_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"McaRe8saWIlo"},"source":["### Applying the model to an image"]},{"cell_type":"markdown","metadata":{"id":"dyInzv7xHe6C"},"source":["Load a sample image"]},{"cell_type":"code","metadata":{"id":"iKRs2tPPHe6E"},"source":["%matplotlib inline\n","import cv2\n","import matplotlib\n","import pylab as plt\n","import numpy as np\n","import util"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZfPn__3He6I"},"source":["test_image = 'sample_images/ski.jpg'\n","\n","oriImg = cv2.imread(test_image) # B,G,R order\n","plt.figure(figsize=(12,12))\n","plt.imshow(oriImg[:,:,[2,1,0]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2R4dc_6PHe6e"},"source":["Load configuration"]},{"cell_type":"code","metadata":{"id":"q-6TBHKyHe6f"},"source":["param, model_params = config_reader()\n","\n","multiplier = [x * model_params['boxsize'] / oriImg.shape[0] for x in param['scale_search']]\n","print ('scale factors for this image:', multiplier)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6AtwsQwqHe6x"},"source":["The following code runs the model on the same image at different scales and aggregates the resulting heatmaps of body part locations (keypoints), and part affinities (PAF) (which encode the degree of association between keypoints).\n","\n","Then, shows sample heatmaps for right elbow and paf for right wrist and right elbow"]},{"cell_type":"code","metadata":{"id":"wE-ASuPsHe6y"},"source":["heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))\n","paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))\n","# first figure shows padded images\n","f, axarr = plt.subplots(1, len(multiplier))\n","f.set_size_inches((20, 5))\n","# second figure shows heatmaps\n","f2, axarr2 = plt.subplots(1, len(multiplier))\n","f2.set_size_inches((20, 5))\n","# third figure shows PAFs\n","f3, axarr3 = plt.subplots(2, len(multiplier))\n","f3.set_size_inches((20, 10))\n","\n","for m in range(len(multiplier)):\n","    scale = multiplier[m]\n","    imageToTest = cv2.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n","    imageToTest_padded, pad = util.padRightDownCorner(imageToTest, model_params['stride'], model_params['padValue'])\n","    axarr[m].imshow(imageToTest_padded[:,:,[2,1,0]])\n","    axarr[m].set_title('Input image: scale %d' % m)\n","\n","    input_img = np.transpose(np.float32(imageToTest_padded[:,:,:,np.newaxis]), (3,0,1,2)) # required shape (1, width, height, channels)\n","    print(\"Input shape: \" + str(input_img.shape))\n","\n","    output_blobs = model.predict(input_img)\n","    print(\"Output shape (heatmap): \" + str(output_blobs[1].shape))\n","\n","    # extract outputs, resize, and remove padding\n","    heatmap = np.squeeze(output_blobs[1]) # output 1 is heatmaps\n","    heatmap = cv2.resize(heatmap, (0,0), fx=model_params['stride'], fy=model_params['stride'], interpolation=cv2.INTER_CUBIC)\n","    heatmap = heatmap[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n","    heatmap = cv2.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n","\n","    paf = np.squeeze(output_blobs[0]) # output 0 is PAFs\n","    paf = cv2.resize(paf, (0,0), fx=model_params['stride'], fy=model_params['stride'], interpolation=cv2.INTER_CUBIC)\n","    paf = paf[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n","    paf = cv2.resize(paf, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n","\n","    # visualization\n","    axarr2[m].imshow(oriImg[:,:,[2,1,0]])\n","    ax2 = axarr2[m].imshow(heatmap[:,:,3], alpha=.5) # right elbow\n","    axarr2[m].set_title('Heatmaps (Relb): scale %d' % m)\n","\n","    axarr3.flat[m].imshow(oriImg[:,:,[2,1,0]])\n","    ax3x = axarr3.flat[m].imshow(paf[:,:,16], alpha=.5) # right elbow\n","    axarr3.flat[m].set_title('PAFs (x comp. of Rwri to Relb): scale %d' % m)\n","    axarr3.flat[len(multiplier) + m].imshow(oriImg[:,:,[2,1,0]])\n","    ax3y = axarr3.flat[len(multiplier) + m].imshow(paf[:,:,17], alpha=.5) # right wrist\n","    axarr3.flat[len(multiplier) + m].set_title('PAFs (y comp. of Relb to Rwri): scale %d' % m)\n","\n","    heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n","    paf_avg = paf_avg + paf / len(multiplier)\n","\n","f2.subplots_adjust(right=0.93)\n","cbar_ax = f2.add_axes([0.95, 0.15, 0.01, 0.7])\n","_ = f2.colorbar(ax2, cax=cbar_ax)\n","\n","f3.subplots_adjust(right=0.93)\n","cbar_axx = f3.add_axes([0.95, 0.57, 0.01, 0.3])\n","_ = f3.colorbar(ax3x, cax=cbar_axx)\n","cbar_axy = f3.add_axes([0.95, 0.15, 0.01, 0.3])\n","_ = f3.colorbar(ax3y, cax=cbar_axy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJoKvbfsHe6_"},"source":["Heatmap for right knee. Note that the body part is encoded in the 3th channel so in this case right knee is\n","at index 9. All body parts are defined in config:\n","part_str = [nose, neck, Rsho, Relb, Rwri, Lsho, Lelb, Lwri, Rhip, Rkne, Rank, Lhip, Lkne, Lank, Leye, Reye, Lear, Rear, pt19]\n","\n","<!-- list(enumerate('nose, neck, Rsho, Relb, Rwri, Lsho, Lelb, Lwri, Rhip, Rkne, Rank, Lhip, Lkne, Lank, Leye, Reye, Lear, Rear, pt19'.split(', ')))-->"]},{"cell_type":"markdown","metadata":{"id":"dlCTl77vc-L5"},"source":["<!--![alt text](https://github.com/CMU-Perceptual-Computing-Lab/openpose/raw/master/doc/media/keypoints_pose_18.png)-->\n","![alt text](data:image/png;base64,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)"]},{"cell_type":"code","metadata":{"id":"j-5rU4r2e1Kf"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XGdHDrlVkA7a"},"source":["### Visualize keypoint heat maps and pafs"]},{"cell_type":"code","metadata":{"id":"s3knAF7FHe7A"},"source":["plt.imshow(oriImg[:,:,[2,1,0]])\n","plt.imshow(heatmap_avg[:,:,9], alpha=.5)\n","fig = matplotlib.pyplot.gcf()\n","cax = matplotlib.pyplot.gca()\n","fig.set_size_inches(12, 12)\n","fig.subplots_adjust(right=0.93)\n","cbar_ax = fig.add_axes([0.95, 0.15, 0.01, 0.7])\n","_ = fig.colorbar(ax2, cax=cbar_ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FnTVQBNgHe7D"},"source":["paf vectors for right elbow and right wrist"]},{"cell_type":"code","metadata":{"id":"xrNK-rsdHe7Q"},"source":["from numpy import ma\n","U = paf_avg[:,:,16] * -1\n","V = paf_avg[:,:,17]\n","X, Y = np.meshgrid(np.arange(U.shape[1]), np.arange(U.shape[0]))\n","M = np.zeros(U.shape, dtype='bool')\n","M[U**2 + V**2 < 0.5 * 0.5] = True\n","U = ma.masked_array(U, mask=M)\n","V = ma.masked_array(V, mask=M)\n","\n","# 1\n","plt.figure()\n","plt.imshow(oriImg[:,:,[2,1,0]], alpha = .5)\n","s = 5\n","Q = plt.quiver(X[::s,::s], Y[::s,::s], U[::s,::s], V[::s,::s],\n","               scale=50, headaxislength=4, alpha=.5, width=0.001, color='r')\n","\n","fig = matplotlib.pyplot.gcf()\n","fig.set_size_inches(12, 12)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yuNgcdsyHe7c"},"source":["### Detect the body parts (keypoints) in the heat maps and visualize them\n","\n","Visualise all detected body parts. Note that we use peaks in heatmaps"]},{"cell_type":"code","metadata":{"id":"hr5nmphEHe7d"},"source":["from scipy.ndimage.filters import gaussian_filter\n","all_peaks = []\n","peak_counter = 0\n","\n","for part in range(19-1):\n","    map_ori = heatmap_avg[:,:,part]\n","    map = gaussian_filter(map_ori, sigma=3)\n","\n","    map_left = np.zeros(map.shape)\n","    map_left[1:,:] = map[:-1,:]\n","    map_right = np.zeros(map.shape)\n","    map_right[:-1,:] = map[1:,:]\n","    map_up = np.zeros(map.shape)\n","    map_up[:,1:] = map[:,:-1]\n","    map_down = np.zeros(map.shape)\n","    map_down[:,:-1] = map[:,1:]\n","\n","    peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > param['thre1']))\n","    peaks = list(zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])) # note reverse\n","    peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n","    id = range(peak_counter, peak_counter + len(peaks))\n","    peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n","\n","    all_peaks.append(peaks_with_score_and_id)\n","    peak_counter += len(peaks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKS1JyERkvDG"},"source":["# visualize\n","colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n","          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n","          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n","cmap = matplotlib.cm.get_cmap('hsv')\n","\n","canvas = cv2.imread(test_image) # B,G,R order\n","\n","for i in range(18):\n","    rgba = np.array(cmap(1 - i/18. - 1./36))\n","    rgba[0:3] *= 255\n","    for j in range(len(all_peaks[i])):\n","        cv2.circle(canvas, all_peaks[i][j][0:2], 4, colors[i], thickness=-1)\n","\n","to_plot = cv2.addWeighted(oriImg, 0.3, canvas, 0.7, 0)\n","plt.imshow(to_plot[:,:,[2,1,0]])\n","fig = matplotlib.pyplot.gcf()\n","fig.set_size_inches(12, 12)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SKWBFCmkx8u"},"source":["### Link the body parts and visualize them"]},{"cell_type":"code","metadata":{"id":"NpziLO78He7k"},"source":["# find connection in the specified sequence, center 29 is in the position 15\n","limbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n","           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n","           [1,16], [16,18], [3,17], [6,18]]\n","# the middle joints heatmap correpondence\n","mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n","          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n","          [55,56], [37,38], [45,46]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rwxx_KOPHe7o"},"source":["connection_all = []\n","special_k = []\n","mid_num = 10\n","\n","for k in range(len(mapIdx)):\n","    score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n","    candA = all_peaks[limbSeq[k][0]-1]\n","    candB = all_peaks[limbSeq[k][1]-1]\n","    nA = len(candA)\n","    nB = len(candB)\n","    indexA, indexB = limbSeq[k]\n","    if(nA != 0 and nB != 0):\n","        connection_candidate = []\n","        for i in range(nA):\n","            for j in range(nB):\n","                vec = np.subtract(candB[j][:2], candA[i][:2])\n","                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n","                # failure case when 2 body parts overlaps\n","                if norm == 0:\n","                    continue\n","                vec = np.divide(vec, norm)\n","\n","                startend = list(zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n","                               np.linspace(candA[i][1], candB[j][1], num=mid_num)))\n","\n","                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n","                                  for I in range(len(startend))])\n","                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n","                                  for I in range(len(startend))])\n","\n","                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n","                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n","                criterion1 = len(np.nonzero(score_midpts > param['thre2'])[0]) > 0.8 * len(score_midpts)\n","                criterion2 = score_with_dist_prior > 0\n","                if criterion1 and criterion2:\n","                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n","\n","        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n","        connection = np.zeros((0,5))\n","        for c in range(len(connection_candidate)):\n","            i,j,s = connection_candidate[c][0:3]\n","            if(i not in connection[:,3] and j not in connection[:,4]):\n","                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n","                if(len(connection) >= min(nA, nB)):\n","                    break\n","\n","        connection_all.append(connection)\n","    else:\n","        special_k.append(k)\n","        connection_all.append([])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWg2b4IxHe7t"},"source":["# last number in each row is the total parts number of that person\n","# the second last number in each row is the score of the overall configuration\n","subset = -1 * np.ones((0, 20))\n","candidate = np.array([item for sublist in all_peaks for item in sublist])\n","\n","for k in range(len(mapIdx)):\n","    if k not in special_k:\n","        partAs = connection_all[k][:,0]\n","        partBs = connection_all[k][:,1]\n","        indexA, indexB = np.array(limbSeq[k]) - 1\n","\n","        for i in range(len(connection_all[k])): #= 1:size(temp,1)\n","            found = 0\n","            subset_idx = [-1, -1]\n","            for j in range(len(subset)): #1:size(subset,1):\n","                if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n","                    subset_idx[found] = j\n","                    found += 1\n","\n","            if found == 1:\n","                j = subset_idx[0]\n","                if(subset[j][indexB] != partBs[i]):\n","                    subset[j][indexB] = partBs[i]\n","                    subset[j][-1] += 1\n","                    subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n","            elif found == 2: # if found 2 and disjoint, merge them\n","                j1, j2 = subset_idx\n","                print (\"found = 2\")\n","                membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n","                if len(np.nonzero(membership == 2)[0]) == 0: #merge\n","                    subset[j1][:-2] += (subset[j2][:-2] + 1)\n","                    subset[j1][-2:] += subset[j2][-2:]\n","                    subset[j1][-2] += connection_all[k][i][2]\n","                    subset = np.delete(subset, j2, 0)\n","                else: # as like found == 1\n","                    subset[j1][indexB] = partBs[i]\n","                    subset[j1][-1] += 1\n","                    subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n","\n","            # if find no partA in the subset, create a new subset\n","            elif not found and k < 17:\n","                row = -1 * np.ones(20)\n","                row[indexA] = partAs[i]\n","                row[indexB] = partBs[i]\n","                row[-1] = 2\n","                row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n","                subset = np.vstack([subset, row])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iokcv9gyHe7w"},"source":["# delete some rows of subset which has few parts occur\n","deleteIdx = [];\n","for i in range(len(subset)):\n","    if subset[i][-1] < 4 or subset[i][-2]/subset[i][-1] < 0.4:\n","        deleteIdx.append(i)\n","subset = np.delete(subset, deleteIdx, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bJSF6eacHe74"},"source":["Visualize linked body parts"]},{"cell_type":"code","metadata":{"id":"nyoyNJ_GHe76"},"source":["# visualize 2\n","stickwidth = 4\n","\n","for i in range(17):\n","    for n in range(len(subset)):\n","        index = subset[n][np.array(limbSeq[i])-1]\n","        if -1 in index:\n","            continue\n","        cur_canvas = canvas.copy()\n","        Y = candidate[index.astype(int), 0]\n","        X = candidate[index.astype(int), 1]\n","        mX = np.mean(X)\n","        mY = np.mean(Y)\n","        length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n","        angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n","        polygon = cv2.ellipse2Poly((int(mY),int(mX)), (int(length/2), stickwidth), int(angle), 0, 360, 1)\n","        cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n","        canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n","\n","plt.imshow(canvas[:,:,[2,1,0]])\n","fig = matplotlib.pyplot.gcf()\n","fig.set_size_inches(12, 12)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsInEHOdHe79"},"source":[],"execution_count":null,"outputs":[]}]}